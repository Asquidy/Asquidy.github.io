{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/andrey_fradkin/Box/Box Sync/Reviews_Paper/aux_data/text_data\")\n",
    "\n",
    "from pandas import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.externals import joblib\n",
    "import Tokenizer \n",
    "### Not necessary\n",
    "import tabulate\n",
    "from operator import add\n",
    " \n",
    "\n",
    "tok = Tokenizer.Tokenizer(preserve_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_data = read_table('reviews_of_guest_paper_train_bad_clean.tsv', sep='\\t', index_col=False)\n",
    "bad_data['new_label'] = False\n",
    "\n",
    "rows_nonrec = bad_data['dim_recommends']==False\n",
    "rows_cleanliness_none = bad_data['dim_cleanliness_rating']=='None'\n",
    "rows_comm_none = bad_data['dim_communication_rating']=='None'\n",
    "rows_rules_none = bad_data['dim_respect_house_rules_rating']=='None'\n",
    "\n",
    "bad_data.ix[bad_data.dim_cleanliness_rating == 'None', 'dim_cleanliness_rating'] = '6'\n",
    "bad_data.ix[bad_data.dim_communication_rating == 'None', 'dim_communication_rating'] = '6'\n",
    "bad_data.ix[bad_data.dim_respect_house_rules_rating == 'None', 'dim_respect_house_rules_rating'] = '6'\n",
    "bad_data.ix[:, 'sum_low_ratings'] = np.array([int(int(x) < 4) for x in bad_data['dim_cleanliness_rating']]) + np.array([int(int(x) < 4) for x in bad_data['dim_communication_rating']]) + np.array([int(int(x) < 4) for x in bad_data['dim_respect_house_rules_rating']])\n",
    "low_ratings = bad_data['sum_low_ratings'] >= 2 \n",
    "bad_data = bad_data[rows_nonrec | low_ratings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_data = read_table('reviews_of_guest_paper_train_good_clean.tsv', sep='\\t', index_col=False)\n",
    "all_data = bad_data.append(good_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vect transform\n",
      "fit\n",
      "predict\n",
      "0.01 l1 False 0 0.88832608013 0.00122511627981 4532 4141 789 4532\n",
      "fit\n",
      "predict\n",
      "1 l1 False 0 0.904671956388 0.000802961538881 5585 3088 1068 5585\n",
      "fit\n",
      "predict\n",
      "10 l1 False 0 0.894988548534 0.00142231042054 5774 2899 1665 5774\n",
      "fit\n",
      "predict\n",
      "0.01 l2 False 0 0.90233551545 0.000969760818143 4901 3772 510 4901\n",
      "fit\n",
      "predict\n",
      "1 l2 False 0 0.908600994451 0.000979686061911 5617 3056 938 5617\n",
      "fit\n",
      "predict\n",
      "10 l2 False 0 0.902084535592 0.00134597732637 5748 2925 1313 5748\n",
      "vect transform\n",
      "fit\n",
      "predict\n",
      "0.01 l1 False 250 0.888321516837 0.00123528214552 4535 4138 790 4535\n",
      "fit\n",
      "predict\n",
      "1 l1 False 250 0.902490670324 0.00101479102047 5286 3387 885 5286\n",
      "fit\n",
      "predict\n",
      "10 l1 False 250 0.901943068931 0.000965665920581 5314 3359 936 5314\n",
      "fit\n",
      "predict\n",
      "0.01 l2 False 250 0.899063605722 0.000889519180601 4899 3774 628 4899\n",
      "fit\n",
      "predict\n",
      "1 l2 False 250 0.902431343351 0.000997423189049 5306 3367 907 5306\n",
      "fit\n",
      "predict\n",
      "10 l2 False 250 0.901970447855 0.000964776494742 5315 3358 935 5315\n",
      "vect transform\n",
      "fit\n",
      "predict\n",
      "0.01 l1 False 500 0.888316953544 0.00123920065787 4533 4140 790 4533\n",
      "fit\n",
      "predict\n",
      "1 l1 False 500 0.898461250646 0.000878657394845 5162 3511 914 5162\n",
      "fit\n",
      "predict\n",
      "10 l1 False 500 0.898438435015 0.000856895505344 5177 3496 934 5177\n",
      "fit\n",
      "predict\n",
      "0.01 l2 False 500 0.897087678666 0.000883995707071 4854 3819 690 4854\n",
      "fit\n",
      "predict\n",
      "1 l2 False 500 0.898447562017 0.000854939497917 5174 3499 914 5174\n",
      "fit\n",
      "predict\n",
      "10 l2 False 500 0.898392802086 0.000839017606439 5175 3498 935 5175\n"
     ]
    }
   ],
   "source": [
    "y = all_data['label']\n",
    "for l in (0, 250, 500):\n",
    "    vect= CountVectorizer(ngram_range=(1,3), binary =True, min_df=l, stop_words =\n",
    "    ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n",
    "    print 'vect transform'\n",
    "    X = vect.fit_transform(all_data['dim_overall_comments'].values)\n",
    "    model_outcomes = list()\n",
    "    feature_weights_list = list()\n",
    "    for j in ['l1', 'l2']:\n",
    "        for k in [False]:\n",
    "            for i in [.01, 1, 10]:\n",
    "                logreg = linear_model.LogisticRegression(C=i, penalty=j, fit_intercept=k)\n",
    "                X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "                model = logreg.fit(X_train, y_train)\n",
    "                print 'fit'\n",
    "                y_pred = model.predict(X)\n",
    "                y_score = model.predict_proba(X)\n",
    "                cross_val_scores = cross_validation.cross_val_score(model, X, y, cv=10)\n",
    "                print 'predict'\n",
    "                y_pred_oos = model.predict(X_test)\n",
    "                cm = confusion_matrix(y_test, y_pred_oos)\n",
    "                outcomes = [str(i), j, str(k), str(l), str(cross_val_scores.mean()), str(cross_val_scores.std() / 2), str(cm[0][0]), str(cm[0][1]), str(cm[1][0]), str(cm[0][0])]\n",
    "                model_outcomes.append(outcomes)\n",
    "                print ' '.join(outcomes)\n",
    "                feature_weights = zip(vect.get_feature_names(),model.coef_[0])\n",
    "                feature_weights_list.append(feature_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Estimate Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "these_stop_words =     ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']\n",
    "l = 0\n",
    "vect= CountVectorizer(ngram_range=(1,3), binary =True, min_df=l, stop_words = these_stop_words)\n",
    "X = vect.fit_transform(all_data['dim_overall_comments'].values)\n",
    "j = 'l2'\n",
    "i = .01\n",
    "\n",
    "### Train Model\n",
    "logreg = linear_model.LogisticRegression(C=i, penalty=j, fit_intercept=k)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "predict\n",
      "0.01 l2 False 0 0.90233551545 0.000969760818143 4901 3772 510 4901\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print 'fit'\n",
    "y_pred = model.predict(X)\n",
    "y_score = model.predict_proba(X)\n",
    "cross_val_scores = cross_validation.cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "print 'predict'\n",
    "y_pred_oos = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_oos)\n",
    "outcomes = [str(i), j, str(k), str(l), str(cross_val_scores.mean()), str(cross_val_scores.std() / 2), str(cm[0][0]), str(cm[0][1]), str(cm[1][0]), str(cm[0][0])]\n",
    "model_outcomes.append(outcomes)\n",
    "\n",
    "print ' '.join(outcomes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'excellent', 1.0538274683669768), (u'wonderful', 1.0148791046314602), (u'great', 0.9921718394826663), (u'pleasure', 0.87319629791398667), (u'anytime', 0.80296249350747295), (u'fantastic', 0.79924757443497152), (u'welcome', 0.79695648797692475), (u'awesome', 0.76801413387760309), (u'spotless', 0.70819409567738201)]\n"
     ]
    }
   ],
   "source": [
    "feature_weights = zip(vect.get_feature_names(),model.coef_[0])\n",
    "feature_weights.sort(key = lambda tup: -tup[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good words\n",
      "[(u'excellent', 1.0538274683669768), (u'wonderful', 1.0148791046314602), (u'great', 0.9921718394826663), (u'pleasure', 0.87319629791398667), (u'anytime', 0.80296249350747295), (u'fantastic', 0.79924757443497152), (u'welcome', 0.79695648797692475), (u'awesome', 0.76801413387760309), (u'spotless', 0.70819409567738201), (u'highly', 0.6806122302461709), (u'love', 0.67883015148396086), (u'clean', 0.66762523118788475), (u'delightful', 0.65332319979501363), (u'super', 0.6271507085613649), (u'lovely', 0.62223060123043483), (u'considerate', 0.60106514251930565), (u'amazing', 0.60063714286185732), (u'recommend', 0.54666240771887498), (u'hesitate', 0.54270316441520972)]\n",
      "bad words\n",
      "[(u'dirty', -1.1167948874279456), (u'review', -0.76351666938319007), (u'mess', -0.74648721789755978), (u'hotel', -0.72156845402042913), (u'difficult', -0.70519715608022426), (u'bad', -0.70400302610115861), (u'messy', -0.64218597865381699), (u'ok', -0.6268281814007588), (u'did', -0.58743780542179991), (u'cleaning', -0.58582792147010376), (u'bit', -0.55803142210523926), (u'unfortunately', -0.53618123964659936), (u'overall', -0.51135394348028262), (u'disappointed', -0.49305772759262778), (u'pay', -0.47054178713762007), (u'broken', -0.46580790491029167), (u'check', -0.46347061950044699), (u'demanding', -0.46207261082080681), (u'comment', -0.45953314809667983), (u'read', -0.45911414247700305), (u'floor', -0.45286056314387246), (u'complained', -0.44158626344117541), (u'think', -0.43257117476798029), (u'bathroom', -0.43048271988891407), (u'kitchen', -0.42688849681287194), (u'worst', -0.42575686515732825), (u'smoking', -0.41843079793330851), (u'negative', -0.41838261494926748), (u'rude', -0.40230580795713822), (u'enthusiastic communicated', -0.39597597267730184), (u'late', -0.39397305536628136), (u'expectations', -0.3929359672141301), (u'guest enthusiastic communicated', -0.3880244990188661), (u'does', -0.38795703105126245), (u'damage', -0.38596404469102835), (u'great guest enthusiastic', -0.38216306532767763), (u'hard', -0.37998871815958413), (u'sorry', -0.37451535175941741), (u'understand', -0.3737908228131907), (u'hours', -0.37036495663277313)]\n"
     ]
    }
   ],
   "source": [
    "print 'good words'\n",
    "print feature_weights[1:20]\n",
    "num_features = len(feature_weights)\n",
    "print 'bad words'\n",
    "print feature_weights[(num_features-40):num_features][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_reg_model_rev_of_guest.pkl',\n",
       " 'log_reg_model_rev_of_guest.pkl_01.npy',\n",
       " 'log_reg_model_rev_of_guest.pkl_02.npy',\n",
       " 'log_reg_model_rev_of_guest.pkl_03.npy',\n",
       " 'log_reg_model_rev_of_guest.pkl_04.npy']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'log_reg_model_rev_of_guest.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vocabulary_rev_of_guest.pkl']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feature_list = vect.get_feature_names()\n",
    "joblib.dump(feature_list, 'vocabulary_rev_of_guest.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
